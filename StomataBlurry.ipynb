{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The pipeline of only use the blurry Stomata to train the Yolov8\n",
    "\n",
    "Only use classes Stomata.blurry and Stomata complete and blurry complete .\n",
    "\n",
    "Degin a dataaugment to let the model work better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import ultralytics\n",
    "import torch\n",
    "import utils\n",
    "from IPython.display import Image  # for displaying images\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ImageDraw ,ImageFont\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cancel to detect hair , aim to detect blurry object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "class_id_to_name_mapping = dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys()))\n",
    "\n",
    "\n",
    "\n",
    "model = YOLO(\"yolov8m.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check ultralytics Yolo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ultralytics.checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change Pascal format to Yolo format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name = \"AllData\"\n",
    "\n",
    "\n",
    "\n",
    "# Function to get the data from XML Annotation\n",
    "def extract_info_from_xml(xml_file):\n",
    "    root = ET.parse(xml_file).getroot()\n",
    "\n",
    "    # Initialise the info dict\n",
    "    info_dict = {}\n",
    "    info_dict['bboxes'] = []\n",
    "    \n",
    "    for elem in root:\n",
    "        # Get the file name\n",
    "        if elem.tag == \"filename\":\n",
    "            info_dict['filename'] = elem.text\n",
    "\n",
    "        # Get the image size\n",
    "        elif elem.tag == \"size\":\n",
    "            image_size = []\n",
    "            for subelem in elem:\n",
    "                image_size.append(int(subelem.text))\n",
    "\n",
    "            info_dict['image_size'] = tuple(image_size)\n",
    "\n",
    "        # Get details of the bounding box\n",
    "        elif elem.tag == \"object\":\n",
    "            bbox = {}\n",
    "            for subelem in elem:\n",
    "                if subelem.tag == \"name\":\n",
    "                    bbox[\"class\"] = subelem.text\n",
    "\n",
    "                elif subelem.tag == \"robndbox\":\n",
    "                    for subsubelem in subelem:\n",
    "                        bbox[subsubelem.tag] = int(float(subsubelem.text))\n",
    "            info_dict['bboxes'].append(bbox)\n",
    "\n",
    "    return info_dict\n",
    "\n",
    "\n",
    "#xml choose complete\n",
    "\n",
    "class_name_to_id_mapping = {\"complete\": 0,\n",
    "                #\"incomplete\": 0,\n",
    "                           \"blurry.complete\": 1,\n",
    "                           #\"blurry.incomplete\" :1 , \n",
    "                           \"hair\":2 } \n",
    "\n",
    "# change robndbox to yolov5 format\n",
    "def convert_to_yolov8(info_dict) :\n",
    "    print_buffer = []\n",
    "\n",
    "    # For each bounding box\n",
    "    for b in info_dict[\"bboxes\"]:\n",
    "\n",
    "        # If the class is not in the pre-defined classes, then ignore it.\n",
    "        if b['classs'] != 'complete' and b['classs'] != 'blurry.complete':\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
    "        except KeyError:\n",
    "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
    "\n",
    "        \n",
    "        # robndbox format\n",
    "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
    "        b_center_x = int(b[\"cx\"])\n",
    "        b_center_y = int(b[\"cy\"])\n",
    "        b_width = int(b[\"w\"])\n",
    "        b_height = int(b[\"h\"])\n",
    "\n",
    "        # Normalise the co-ordinates by the dimensions of the image\n",
    "        image_w, image_h, image_c = info_dict[\"image_size\"]\n",
    "        b_center_x /= image_w\n",
    "        b_center_y /= image_h\n",
    "        b_width    /= image_w\n",
    "        b_height   /= image_h\n",
    "\n",
    "        #Write the bbox details to the file\n",
    "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
    "\n",
    "    # Name of the file which we have to save\n",
    "    save_file_name = os.path.join('./' + dir_name, info_dict[\"filename\"] + '.txt' )\n",
    "\n",
    "    # Save the annotation to disk\n",
    "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the annotationse\n",
    "annotations = [os.path.join('./' + dir_name, x) for x in os.listdir('./'+dir_name ) if x[-3:] == \"xml\"]\n",
    "\n",
    "\n",
    "annotations.sort()\n",
    "#print(annotations)\n",
    "# Convert and save the annotations\n",
    "for ann in tqdm(annotations):\n",
    "    info_dict = extract_info_from_xml(ann)\n",
    "    convert_to_yolov8(info_dict)\n",
    "    \n",
    "annotations = [os.path.join('./' + dir_name, x) for x in os.listdir('./' + dir_name) if x[-3:] == \"txt\"]\n",
    "\n",
    "print(annotations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the annotations with only on blurry\n",
    "\n",
    "## Label the class on the image\n",
    "Just for a sanity check, let us now test some of these transformed annotations. We randomly load one of the annotations and plot boxes using the transformed annotations, and visually inspect it to see whether our code has worked as intended.\n",
    "\n",
    "Run the next cell multiple times. Every time, a random annotation is sampled.\n",
    "\n",
    "Use command mv ./T22/*_label.jpg ./T22Label  \n",
    "move label image to folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(0)\n",
    "\n",
    "\n",
    "class_name_to_id_mapping = {\n",
    "    \"complete\": 0,\n",
    "    \"blurry.complete\": 1,\n",
    "                           } # human hair\n",
    "\n",
    "class_id_to_name_mapping = dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys()))\n",
    "\n",
    "fnt = ImageFont.truetype(\"Pillow/Tests/fonts/FreeMono.ttf\", 40)\n",
    "\n",
    "\n",
    "\n",
    "def PlotAndSavebounding_box(image_name,image, annotation_list):\n",
    "    annotations = np.array(annotation_list)\n",
    "    w, h = image.size\n",
    "\n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "\n",
    "    transformed_annotations = np.copy(annotations)\n",
    "    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w\n",
    "    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h\n",
    "\n",
    "    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)\n",
    "    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)\n",
    "    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]\n",
    "    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]\n",
    "\n",
    "    for ann in transformed_annotations:\n",
    "        obj_cls, x0, y0, x1, y1 = ann\n",
    "        plotted_image.rectangle(((x0,y0), (x1,y1)),outline =\"red\", width = 3)\n",
    "        plotted_image.text((x0, y0-10), class_id_to_name_mapping[(int(obj_cls))] , fill = (255,0,255,255), font = fnt)\n",
    "    \n",
    "    # covert PIL image to cv2 image\n",
    "    open_cv_image = np.array(image)\n",
    "\n",
    "    # Convert RGB to BGR\n",
    "    open_cv_image = open_cv_image[:, :, ::-1].copy()\n",
    "\n",
    "    print(type(image))\n",
    "\n",
    "\n",
    "    image_name = image_name.replace(\".jpg\", \"_label.jpg\")\n",
    "    \n",
    "    cv2.imwrite( image_name , open_cv_image, [cv2.IMWRITE_JPEG_QUALITY, 100])\n",
    "\n",
    "    #plt.imshow(np.array(image))\n",
    "    #plt.show()\n",
    "\n",
    "\n",
    "# Get any random annotation file\n",
    "#annotation_file = annotations[0]\n",
    "\n",
    "'''\n",
    "with open(annotation_file, \"r\") as file:\n",
    "    annotation_list = file.read().split(\"\\n\")[:-1]\n",
    "    annotation_list = [x.split(\" \") for x in annotation_list]\n",
    "    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
    "'''    \n",
    "\n",
    "\n",
    "# use %cd T22Label to locate folder\n",
    "\n",
    "# Plot the Bounding Box\n",
    "# Polt all the Images Labels and annotations\n",
    "for i in range(0,len(annotations)):\n",
    "\n",
    "    annotation_file = annotations[i]\n",
    "\n",
    "    with open(annotation_file, \"r\") as file:\n",
    "        annotation_list = file.read().split(\"\\n\")[:-1]\n",
    "        annotation_list = [x.split(\" \") for x in annotation_list]\n",
    "        annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
    "\n",
    "    #Get the corresponding image file\n",
    "    image_file = annotation_file.replace(\"txt\", \"jpg\")\n",
    "    \n",
    "    assert os.path.exists(image_file)\n",
    "\n",
    "    #Load the image\n",
    "\n",
    "    image = Image.open(image_file)\n",
    "    PlotAndSavebounding_box(image_file,image, annotation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read Image and Annotation\n",
    "###   Split to train datasets and Test data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images and annotations\n",
    "images = [os.path.join('./' + dir_name, x) for x in os.listdir('./' + dir_name) if x[-3:] == 'jpg' ]\n",
    "annotations = [os.path.join('./' + dir_name, x) for x in os.listdir('./' + dir_name) if x[-3:] == \"txt\"]\n",
    "\n",
    "images.sort()\n",
    "annotations.sort()\n",
    "\n",
    "# Split the dataset into train-valid-test splits\n",
    "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
    "val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './' + dir_name\n",
    "#!mkdir ./T32/images ./T32/labels\n",
    "\n",
    "os.mkdir(path + '/images')\n",
    "os.mkdir(path + '/labels')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!mkdir ./T32/images/train ./T32/images/val ./T32/images/test  ./T32/labels/train  ./T32/labels/val  ./T32/labels/test\n",
    "\n",
    "os.mkdir(path + '/images/train')\n",
    "os.mkdir(path + '/images/val')\n",
    "os.mkdir(path + '/images/test')\n",
    "os.mkdir(path + '/labels/train')\n",
    "os.mkdir(path + '/labels/val')\n",
    "os.mkdir(path + '/labels/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create a new file called road_sign_data.yaml and place it in the yolov5/data folder. Then populate it with the following.\n",
    "\n",
    "train : ./T22/images/train/\n",
    "val : ./T22/images/val/\n",
    "test : ./T22/images/test/\n",
    "\n",
    "\n",
    "##### number of classes\n",
    "nc: 5\n",
    "\n",
    "##### class names\n",
    "names : [\"complete\", \"blurry.complete\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Yolov8Stomata",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
